{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example model to run for now just to get an idea \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trialing a pipeline \n",
    "\n",
    "Here we want to create a trial pipleine , in this trial pipeline we will have random integers the goal is to get out model to output some scores for a couple pipes basically for now they can be random integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example model to run for now just to get an idea \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TowerOne(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerOne, self).__init__()\n",
    "        \n",
    "        # First layer: input 64 features, output 32 features\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        \n",
    "        # Second layer: input 32 features, output 16 features\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        \n",
    "        # Third layer: input 16 features, output 3 features (final output)\n",
    "        self.fc3 = nn.Linear(16, 3)\n",
    "        \n",
    "        # Activation function (ReLU) applied after first and second layers\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the first layer, followed by ReLU\n",
    "        x = self.relu(self.fc1(x))\n",
    "        \n",
    "        # Pass through the second layer, followed by ReLU\n",
    "        x = self.relu(self.fc2(x))\n",
    "        \n",
    "        # Pass through the final layer to get 3 output features\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TowerTwo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerTwo, self).__init__()\n",
    "        self.fc = nn.Linear(64, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video input: tensor([[0.4139, 0.7380, 0.2999, 0.3325, 0.0293, 0.4814, 0.3391, 0.9069, 0.7763,\n",
      "         0.0498, 0.5203, 0.0422, 0.3766, 0.6510, 0.6568, 0.4529, 0.9977, 0.2522,\n",
      "         0.9066, 0.0753, 0.8177, 0.1161, 0.6997, 0.4761, 0.5777, 0.1587, 0.9614,\n",
      "         0.5546, 0.4156, 0.5668, 0.9692, 0.1272, 0.5269, 0.8872, 0.6158, 0.8411,\n",
      "         0.7299, 0.1364, 0.8546, 0.0254, 0.3160, 0.8907, 0.3916, 0.0663, 0.2259,\n",
      "         0.9908, 0.0894, 0.6401, 0.2493, 0.8152, 0.6204, 0.2751, 0.6881, 0.4009,\n",
      "         0.7173, 0.4172, 0.1987, 0.7279, 0.9791, 0.4464, 0.2679, 0.2666, 0.5608,\n",
      "         0.2206]])\n",
      "Query input: tensor([[0.2647, 0.4808, 0.9866]])\n",
      "Output from TowerOne: tensor([[-0.0077,  0.0032, -0.2654]], grad_fn=<AddmmBackward0>)\n",
      "Output from TowerTwo: tensor([[-0.4474,  0.2700,  0.4150]], grad_fn=<AddmmBackward0>)\n",
      "Cosine similarity score: tensor([-0.5973], grad_fn=<SumBackward1>)\n",
      "Loss: tensor(2.5513, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#call embedder\n",
    "\n",
    "\n",
    "# Generate random inputs\n",
    "video = torch.rand(1, 64)\n",
    "video = torch.rand(1, 64)\n",
    "video = torch.rand(1, 64)\n",
    "video = torch.rand(1, 64)\n",
    "\n",
    "\n",
    "query = torch.rand(1, 3)   # Input for TowerTwo\n",
    "\n",
    "# Initialize models\n",
    "tower_one = TowerOne()\n",
    "tower_two = TowerTwo()\n",
    "\n",
    "# Forward pass through both towers\n",
    "output_one = tower_one(video)  # Pass video through TowerOne\n",
    "output_two = tower_two(query)  # Pass query through TowerTwo\n",
    "\n",
    "# Compute cosine similarity between the outputs\n",
    "score = nn.functional.cosine_similarity(output_one, output_two, dim=1)\n",
    "\n",
    "# Define target and compute loss (mean squared error)\n",
    "target = torch.tensor([1.0])  # The target cosine similarity\n",
    "loss = (score - target).pow(2).mean()\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Print outputs for debugging\n",
    "print(\"Video input:\", video)\n",
    "print(\"Query input:\", query)\n",
    "print(\"Output from TowerOne:\", output_one)\n",
    "print(\"Output from TowerTwo:\", output_two)\n",
    "print(\"Cosine similarity score:\", score)\n",
    "print(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor output: tensor([[-0.1423, -0.1539,  0.1909]], grad_fn=<AddmmBackward0>)\n",
      "Positive 1 output: tensor([[-0.2249,  0.2396,  0.2570]], grad_fn=<AddmmBackward0>)\n",
      "Positive 2 output: tensor([[-0.6124,  0.3555,  0.0086]], grad_fn=<AddmmBackward0>)\n",
      "Negative 1 output: tensor([[-0.4607,  0.0852,  0.0928]], grad_fn=<AddmmBackward0>)\n",
      "Negative 2 output: tensor([[-0.3622,  0.3388,  0.0395]], grad_fn=<AddmmBackward0>)\n",
      "Cosine similarity positive 1: tensor([0.3736], grad_fn=<SumBackward1>)\n",
      "Cosine similarity positive 2: tensor([0.1697], grad_fn=<SumBackward1>)\n",
      "Cosine similarity negative 1: tensor([0.5182], grad_fn=<SumBackward1>)\n",
      "Cosine similarity negative 2: tensor([0.0493], grad_fn=<SumBackward1>)\n",
      "Triplet loss: tensor(2.0242, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define margin for triplet loss\n",
    "margin = 1.0\n",
    "\n",
    "\n",
    "positive_video1 = torch.rand(1, 64)\n",
    "positive_video2 = torch.rand(1, 64)\n",
    "negative_video1 = torch.rand(1, 64)\n",
    "negative_video2 = torch.rand(1, 64)\n",
    "\n",
    "anchor_query = torch.rand(1, 64)  # Input for TowerTwo (optional for your task)\n",
    "\n",
    "# Initialize models\n",
    "tower_one = TowerOne()\n",
    "tower_two = TowerTwo()\n",
    "\n",
    "# Forward pass through the tower for each video\n",
    "anchor_output = tower_one(anchor_query)          # Anchor\n",
    "positive_output1 = tower_two(positive_video1)    # Positive sample 1\n",
    "positive_output2 = tower_two(positive_video2)    # Positive sample 2\n",
    "negative_output1 = tower_two(negative_video1)    # Negative sample 1\n",
    "negative_output2 = tower_two(negative_video2)    # Negative sample 2\n",
    "\n",
    "# Cosine similarity between anchor and positives\n",
    "cosine_sim_positive1 = nn.functional.cosine_similarity(anchor_output, positive_output1, dim=1)\n",
    "cosine_sim_positive2 = nn.functional.cosine_similarity(anchor_output, positive_output2, dim=1)\n",
    "\n",
    "\n",
    "# Cosine similarity between anchor and negatives\n",
    "cosine_sim_negative1 = nn.functional.cosine_similarity(anchor_output, negative_output1, dim=1)\n",
    "cosine_sim_negative2 = nn.functional.cosine_similarity(anchor_output, negative_output2, dim=1)\n",
    "\n",
    "# Compute Triplet Loss (for positive1-negative1 and positive2-negative2)\n",
    "triplet_loss1 = torch.clamp(margin - cosine_sim_positive1 + cosine_sim_negative1, min=0)\n",
    "triplet_loss2 = torch.clamp(margin - cosine_sim_positive2 + cosine_sim_negative2, min=0)\n",
    "\n",
    "# Average the losses\n",
    "triplet_loss = (triplet_loss1 + triplet_loss2).mean()\n",
    "\n",
    "# Backpropagation\n",
    "triplet_loss.backward()\n",
    "\n",
    "# Print debugging information\n",
    "print(\"Anchor output:\", anchor_output)\n",
    "print(\"Positive 1 output:\", positive_output1)\n",
    "print(\"Positive 2 output:\", positive_output2)\n",
    "print(\"Negative 1 output:\", negative_output1)\n",
    "print(\"Negative 2 output:\", negative_output2)\n",
    "print(\"Cosine similarity positive 1:\", cosine_sim_positive1)\n",
    "print(\"Cosine similarity positive 2:\", cosine_sim_positive2)\n",
    "print(\"Cosine similarity negative 1:\", cosine_sim_negative1)\n",
    "print(\"Cosine similarity negative 2:\", cosine_sim_negative2)\n",
    "print(\"Triplet loss:\", triplet_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1000/1000 [00:02<00:00, 423.17epoch/s, Triplet Loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Define the margin for triplet loss\n",
    "margin = 1.0\n",
    "num_epochs = 1000  # Number of iterations/epochs to run\n",
    "learning_rate = 0.001  # Learning rate for optimizer\n",
    "batch_size = 16  # Batch size for training\n",
    "\n",
    "# Initialize models (Make sure TowerOne and TowerTwo are defined)\n",
    "tower_one = TowerOne()\n",
    "tower_two = TowerTwo()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=learning_rate)\n",
    "\n",
    "# TripletMarginWithDistanceLoss with cosine similarity as the distance function\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=margin\n",
    ")\n",
    "\n",
    "# Training loop with tqdm for progress tracking\n",
    "with tqdm(total=num_epochs, desc=\"Training Progress\", unit=\"epoch\") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Generate random batches of video inputs (batch size anchor, 2 positive, 2 negative)\n",
    "        anchor_query = torch.rand(batch_size, 64)    # Anchor batch\n",
    "        positive_video1 = torch.rand(batch_size, 64) # Positive batch 1\n",
    "        positive_video2 = torch.rand(batch_size, 64) # Positive batch 2\n",
    "        negative_video1 = torch.rand(batch_size, 64) # Negative batch 1\n",
    "        negative_video2 = torch.rand(batch_size, 64) # Negative batch 2\n",
    "        \n",
    "        # Zero gradients (reset gradients before backpropagation)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the tower for each video batch\n",
    "        anchor_output = tower_one(anchor_query)          # Anchor\n",
    "        positive_output1 = tower_two(positive_video1)    # Positive sample 1\n",
    "        positive_output2 = tower_two(positive_video2)    # Positive sample 2\n",
    "        negative_output1 = tower_two(negative_video1)    # Negative sample 1\n",
    "        negative_output2 = tower_two(negative_video2)    # Negative sample 2\n",
    "\n",
    "        # Calculate triplet loss using custom cosine similarity distance\n",
    "        triplet_loss1 = triplet_loss_fn(anchor_output, positive_output1, negative_output1)\n",
    "        triplet_loss2 = triplet_loss_fn(anchor_output, positive_output2, negative_output2)\n",
    "\n",
    "        # Average the triplet losses\n",
    "        triplet_loss = (triplet_loss1 + triplet_loss2) / 2\n",
    "\n",
    "        # Backpropagation\n",
    "        triplet_loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm description with current loss\n",
    "        pbar.set_postfix({\"Triplet Loss\": triplet_loss.item()})\n",
    "        pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying out model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example model to run for now just to get an idea \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramFoo(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "  # def forward(self, inpt, trgs, rand):\n",
    "  #   emb = self.emb(inpt)\n",
    "  #   ctx = self.ffw.weight[trgs]\n",
    "  #   rnd = self.ffw.weight[rand]\n",
    "  #   out = torch.mm(ctx, emb.T)\n",
    "  #   rnd = torch.mm(rnd, emb.T)\n",
    "  #   out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "  #   rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "  #   pst = -out.log().mean()\n",
    "  #   ngt = -(1 - rnd).log().mean()\n",
    "  #   return pst + ngt\n",
    "\n",
    "#new forwarding for batch size \n",
    "  def forward(self, inpt, trgs, rand):\n",
    "    # Embedding lookup for input (shape: [batch_size, embedding_dim])\n",
    "    emb = self.emb(inpt)\n",
    "    \n",
    "    # Ensure context (trgs) and random samples (rand) have the same batch size as inpt\n",
    "    batch_size = inpt.size(0)  # Get the current batch size\n",
    "\n",
    "    # Slice or generate the random tensor according to the input batch size\n",
    "    rand = rand[:batch_size]  # Adjust random tensor to match current batch size\n",
    "    \n",
    "    ctx = self.ffw.weight[trgs.to(inpt.device)]  # Shape: [batch_size, 2, embedding_dim]\n",
    "    rnd = self.ffw.weight[rand.to(inpt.device)]  # Shape: [batch_size, 2, embedding_dim]\n",
    "\n",
    "    # Ensure the batch size matches before performing batch matrix multiplication\n",
    "    assert ctx.size(0) == emb.size(0), f\"Context batch size {ctx.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "    assert rnd.size(0) == emb.size(0), f\"Random batch size {rnd.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "    \n",
    "    # Perform batch matrix multiplication\n",
    "    out = torch.bmm(ctx, emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 2)\n",
    "    rnd = torch.bmm(rnd, emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 2)\n",
    "    \n",
    "    # Apply sigmoid and clamp to prevent NaNs\n",
    "    out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "    rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "    # Calculate loss\n",
    "    pst = -out.log().mean()   # Positive sample log-likelihood\n",
    "    ngt = -(1 - rnd).log().mean()  # Negative sample log-likelihood\n",
    "    \n",
    "    return pst + ngt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_26928\\4031393700.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mfoo = torch.load(model_path, map_location=torch.device('cpu'))\n",
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_26928\\4031393700.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mfoo.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkipGramFoo(\n",
       "  (emb): Embedding(70572, 64)\n",
       "  (ffw): Linear(in_features=64, out_features=70572, bias=False)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create an instance of your model\n",
    "model_path = \"C:/Users/omare/Desktop/hackernews_predicting_views/data processing/skipgram_model_titles.pth\"\n",
    "mfoo = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "mfoo = SkipGramFoo(voc=70572, emb=64, ctx=2)  # Initialize with proper params\n",
    "mfoo.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "mfoo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in dataset of passages to add to dictionary \n",
    "extra_voc = pd.read_csv('extra_dict.csv')\n",
    "new_words = extra_voc['passage_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace punctuation with placeholders\n",
    "    text = text.replace('.',  ' <PERIOD> ')\n",
    "    text = text.replace(',',  ' <COMMA> ')\n",
    "    text = text.replace('\"',  ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';',  ' <SEMICOLON> ')\n",
    "    text = text.replace('!',  ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(',  ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')',  ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace(':',  ' <COLON> ')\n",
    "\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Count occurrences of each word\n",
    "    stats = collections.Counter(words)\n",
    "    \n",
    "    # Filter words to keep only those that occur more than 15 times\n",
    "    words = [word for word in words if stats[word] > 15]\n",
    "    \n",
    "    # Remove any remaining symbols (non-alphabetic characters)\n",
    "    words = [word for word in words if re.match(r'^[a-zA-Z<>\\s]+$', word)]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Convert the Pandas Series to a list\n",
    "new_words = new_words.tolist()  \n",
    "\n",
    "# Now call your preprocess function\n",
    "new_words = preprocess(' '.join(new_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols found: Counter({'<': 6075090, '>': 6075090})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def identify_symbols(text) -> list[str]:\n",
    "    # If text is a list, join it into a single string\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    # Use regex to find non-alphabetic characters\n",
    "    symbols = re.findall(r'[^a-zA-Z\\s]', text)\n",
    "    \n",
    "    # Count the occurrences of each symbol\n",
    "    symbol_counts = collections.Counter(symbols)\n",
    "    \n",
    "    return symbol_counts\n",
    "\n",
    "\n",
    "# Identify symbols in the list of words\n",
    "symbols = identify_symbols(new_words)\n",
    "\n",
    "print(\"Symbols found:\", symbols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70572\n",
      "121598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load existing vocabulary\n",
    "import pickle\n",
    "\n",
    "with open(\"vocab_dict.pkl\", \"rb\") as f:\n",
    "    vocab_dict = pickle.load(f)\n",
    "\n",
    "print(len(vocab_dict))\n",
    "\n",
    "for word in new_words:\n",
    "    if word not in vocab_dict:\n",
    "        vocab_dict[word] = len(vocab_dict)  # Assign new index\n",
    "\n",
    "print(len(vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicated words found.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Invert vocab_dict to detect duplicates (word -> list of IDs)\n",
    "inverted_vocab = defaultdict(list)\n",
    "\n",
    "# Fill the inverted dictionary (key: word, value: list of IDs)\n",
    "for word_id, word in vocab_dict.items():\n",
    "    inverted_vocab[word].append(word_id)\n",
    "\n",
    "# Check for duplicated words\n",
    "duplicated_words = {word: ids for word, ids in inverted_vocab.items() if len(ids) > 1}\n",
    "\n",
    "# Display the duplicated words and their IDs\n",
    "if duplicated_words:\n",
    "    print(\"Duplicated Words Found:\")\n",
    "    for word, ids in duplicated_words.items():\n",
    "        print(f\"Word: '{word}' has multiple IDs: {ids}\")\n",
    "else:\n",
    "    print(\"No duplicated words found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of token IDs corresponding to the new words\n",
    "new_token_ids = []\n",
    "\n",
    "for word in new_words:\n",
    "    if word in vocab_dict:\n",
    "        new_token_ids.append(vocab_dict[word])  # Get the ID from the vocabulary dictionary\n",
    "    else:\n",
    "        print(f\"Warning: '{word}' not found in the vocabulary dictionary.\")  # Optional: handle missing words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Vocabulary Size: 70572\n",
      "New Vocabulary Size: 121598\n"
     ]
    }
   ],
   "source": [
    "# Check sizes\n",
    "print(f\"Old Vocabulary Size: {old_vocab_size}\")\n",
    "print(f\"New Vocabulary Size: {new_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming mfoo is already defined and is your SkipGram model\n",
    "\n",
    "# Get old vocabulary size and embedding dimension\n",
    "old_vocab_size = 70572  # Your old vocab size\n",
    "new_vocab_size = 121598 # Your new vocab size\n",
    "embedding_dim = mfoo.emb.embedding_dim  # Get the embedding dimension\n",
    "\n",
    "# Create a new embedding layer with the new vocabulary size\n",
    "new_embedding_layer = torch.nn.Embedding(new_vocab_size, embedding_dim)\n",
    "\n",
    "# Copy old weights to the new layer, ensuring you do not exceed the size\n",
    "if old_vocab_size <= new_vocab_size:\n",
    "    new_embedding_layer.weight.data[:old_vocab_size] = mfoo.emb.weight.data[:old_vocab_size]\n",
    "\n",
    "# Initialize the new weights for the newly added words (if desired)\n",
    "new_embedding_layer.weight.data[old_vocab_size:] = torch.randn(new_vocab_size - old_vocab_size, embedding_dim)\n",
    "\n",
    "# Replace the old embedding layer with the new one\n",
    "mfoo.emb = new_embedding_layer\n",
    "\n",
    "# Update the Linear Layer (self.ffw) to handle the new vocabulary size\n",
    "new_ffw_layer = torch.nn.Linear(in_features=embedding_dim, out_features=new_vocab_size, bias=False)\n",
    "\n",
    "# Copy the old weights to the new linear layer\n",
    "new_ffw_layer.weight.data[:old_vocab_size] = mfoo.ffw.weight.data[:old_vocab_size]\n",
    "\n",
    "# Initialize the new weights for the new vocabulary\n",
    "new_ffw_layer.weight.data[old_vocab_size:] = torch.randn(new_vocab_size - old_vocab_size, embedding_dim)\n",
    "\n",
    "# Replace the old Linear layer (self.ffw) with the new one\n",
    "mfoo.ffw = new_ffw_layer\n",
    "\n",
    "# Now your model (mfoo) is updated and ready for fine-tuning with the new vocabulary size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xcpmr9u2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>█▂▂▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▅▄▄▃▃▂▂▃▂▂▂▂▂▂▂▁▁▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>0.30825</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss</td><td>0.27672</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine_tuning_skipgram</strong> at: <a href='https://wandb.ai/omareweis123/skipgram_training/runs/xcpmr9u2' target=\"_blank\">https://wandb.ai/omareweis123/skipgram_training/runs/xcpmr9u2</a><br/> View project at: <a href='https://wandb.ai/omareweis123/skipgram_training' target=\"_blank\">https://wandb.ai/omareweis123/skipgram_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_180106-xcpmr9u2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xcpmr9u2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\two_towers\\wandb\\run-20241022_182025-smrptwbv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/skipgram_training/runs/smrptwbv' target=\"_blank\">fine_tuning_skipgram/final/10ep</a></strong> to <a href='https://wandb.ai/omareweis123/skipgram_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/skipgram_training' target=\"_blank\">https://wandb.ai/omareweis123/skipgram_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/skipgram_training/runs/smrptwbv' target=\"_blank\">https://wandb.ai/omareweis123/skipgram_training/runs/smrptwbv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▆▃█▆▄▄▇▅▄▇▄▆█▅▃▆▅▄▅▆▆█▄▆▁▃▂▆▅▆█▄█▄▅▂▅▄▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>0.26334</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss</td><td>0.24762</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine_tuning_skipgram/final/10ep</strong> at: <a href='https://wandb.ai/omareweis123/skipgram_training/runs/smrptwbv' target=\"_blank\">https://wandb.ai/omareweis123/skipgram_training/runs/smrptwbv</a><br/> View project at: <a href='https://wandb.ai/omareweis123/skipgram_training' target=\"_blank\">https://wandb.ai/omareweis123/skipgram_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241022_182025-smrptwbv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import more_itertools\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"skipgram_training\", entity=\"omareweis123\", name='fine_tuning_skipgram/final/10ep')\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 3000\n",
    "learning_rate = 0.001 # Define your learning rate\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mFoo = mfoo.to(device)\n",
    "\n",
    "# Set context size\n",
    "context_size = 2  # Example context size\n",
    "window_size = 2 * context_size + 1  # Total tokens in the window\n",
    "\n",
    "# Initialize the optimizer\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Create your new dataset (make sure tokens are prepared)\n",
    "# Assume new_tokens is your new dataset's tokenized input\n",
    "wins = list(more_itertools.windowed(new_token_ids, window_size))  # Create sliding windows for the new dataset\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    prgs = tqdm.tqdm(range(0, len(wins), batch_size), total=len(wins) // batch_size, desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    total_loss = 0.0  # Initialize total loss for the epoch\n",
    "    num_batches = 0   # Counter for batches\n",
    "\n",
    "    for batch_idx in prgs:\n",
    "        batch_wins = wins[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        # Prepare batch inputs and targets\n",
    "        inpts = torch.LongTensor([win[context_size] for win in batch_wins]).to(device)  # Central token for each window\n",
    "        trgs = torch.LongTensor([[win[i] for i in range(context_size)] + [win[i] for i in range(context_size + 1, window_size)]\n",
    "                                  for win in batch_wins]).to(device)  # Context tokens (left and right)\n",
    "        rand = torch.randint(0, len(vocab_dict), (batch_size, 2)).to(device)  # Random negative samples\n",
    "\n",
    "        # Zero gradients\n",
    "        opFoo.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss = mFoo(inpts, trgs, rand)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        opFoo.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Log the loss\n",
    "        wandb.log({'loss': loss.item(), 'learning_rate': learning_rate})\n",
    "\n",
    "    # Calculate and log average loss for the epoch\n",
    "    average_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    wandb.log({'average_loss': average_loss})\n",
    "\n",
    "# Finish the W&B logging\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the file path where you want to save the model\n",
    "model_save_path = \"finetuned_skipgram_model.pth\"\n",
    "\n",
    "# Save entire model\n",
    "torch.save({\n",
    "    'model_state_dict': mFoo.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, \"finetuned_skipgram_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the two tower model on the finetune embedder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TowerOneRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerOneRNN, self).__init__()\n",
    "        \n",
    "        # Single LSTM layer: input 64 features, hidden size 32\n",
    "        self.rnn = nn.RNN(input_size=64, hidden_size=32, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map from 32 features to 3 (final output)\n",
    "        self.fc = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the LSTM layer\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # The LSTM outputs sequences; get the output from the last time step\n",
    "        x = x[:, -1, :]  # Keep the output for the last time step\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TowerTwoRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerTwoRNN, self).__init__()  # Corrected to use TowerTwoRNN\n",
    "        \n",
    "        # Single LSTM layer: input 64 features, hidden size 32\n",
    "        self.rnn = nn.RNN(input_size=64, hidden_size=32, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map from 32 features to 3 (final output)\n",
    "        self.fc = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the LSTM layer\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # The LSTM outputs sequences; get the output from the last time step\n",
    "        x = x[:, -1, :]  # Keep the output for the last time step\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_26928\\793611519.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mFoo.load_state_dict(torch.load(model_path), strict=False)  # Use strict=False to allow for flexible loading\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume `updated_vocab` is your vocabulary dictionary\n",
    "updated_vocab = vocab_dict  # Replace with your actual vocab dictionary\n",
    "\n",
    "# Step 1: Load the dataframe\n",
    "valid_titles_df = pd.read_csv('extra_dict.csv')  # Assuming the file is valid_titles.csv\n",
    "titles = valid_titles_df['query'].tolist()\n",
    "\n",
    "# Function to create a reverse vocabulary from the updated vocabulary\n",
    "def create_reverse_vocab(vocab):\n",
    "    return {word: index for index, word in vocab.items()}\n",
    "\n",
    "# Load the reverse vocabulary\n",
    "reverse_vocab = create_reverse_vocab(updated_vocab)\n",
    "\n",
    "# Tokenize the titles using the reverse vocabulary\n",
    "def tokenize_titles(titles, reverse_vocab):\n",
    "    tokens = []\n",
    "    \n",
    "    for title in titles:\n",
    "        words = title.lower().split()  # Convert the title to lowercase to match training preprocessing\n",
    "        \n",
    "        tokenized = []\n",
    "        for word in words:\n",
    "            if word in reverse_vocab:\n",
    "                tokenized.append(reverse_vocab[word])  # Get the index from reverse_vocab\n",
    "            # No else clause needed; unknown words are simply skipped\n",
    "        \n",
    "        tokens.append(tokenized)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Now we use the reverse_vocab to tokenize the titles\n",
    "tokenized_titles = tokenize_titles(titles, reverse_vocab)\n",
    "\n",
    "# Step 3: Define the SkipGramFoo model\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "    def __init__(self, voc, emb, ctx):\n",
    "        super().__init__()\n",
    "        self.ctx = ctx\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "        self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inpt, trgs, rand):\n",
    "        emb = self.emb(inpt)\n",
    "        batch_size = inpt.size(0)  # Get the current batch size\n",
    "        rand = rand[:batch_size]  # Adjust random tensor to match current batch size\n",
    "        \n",
    "        ctx = self.ffw.weight[trgs.to(inpt.device)]  # Shape: [batch_size, embedding_dim]\n",
    "        rnd = self.ffw.weight[rand.to(inpt.device)]  # Shape: [batch_size, embedding_dim]\n",
    "\n",
    "        # Perform batch matrix multiplication\n",
    "        out = torch.bmm(ctx.view(batch_size, 1, -1), emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 1)\n",
    "        rnd = torch.bmm(rnd.view(batch_size, 1, -1), emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Apply sigmoid and clamp to prevent NaNs\n",
    "        out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "        rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "        # Calculate loss\n",
    "        pst = -out.log().mean()   # Positive sample log-likelihood\n",
    "        ngt = -(1 - rnd).log().mean()  # Negative sample log-likelihood\n",
    "        \n",
    "        return pst + ngt\n",
    "\n",
    "# Step 4: Load the trained SkipGram model\n",
    "model_path = \"finetuned_skipgram_model.pth\"\n",
    "embedding_dim = 64  # Set to the same dimension used when training\n",
    "\n",
    "# Ensure the model is moved to the GPU\n",
    "mFoo = SkipGramFoo(len(updated_vocab), embedding_dim, 2).to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "try:\n",
    "    mFoo.load_state_dict(torch.load(model_path), strict=False)  # Use strict=False to allow for flexible loading\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading state dict: {e}\")\n",
    "\n",
    "mFoo.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Step 5: Generate embeddings for each title\n",
    "def get_embeddings_for_titles(tokenized_titles, model):\n",
    "    embeddings_list = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations for faster performance\n",
    "        for tokens in tokenized_titles:\n",
    "            if len(tokens) > 0:\n",
    "                # Move the tokens to the GPU\n",
    "                token_tensor = torch.LongTensor(tokens).to(device)\n",
    "                \n",
    "                # Get the embeddings for each token in the title\n",
    "                token_embeddings = model.emb(token_tensor)  # Shape: [num_tokens, embedding_dim]\n",
    "                \n",
    "                # Average the token embeddings to get a single vector for the entire title\n",
    "                title_embedding = token_embeddings.mean(dim=0)  # Shape: [embedding_dim]\n",
    "                \n",
    "                embeddings_list.append(title_embedding.cpu().numpy())  # Store the embedding as a NumPy array\n",
    "            else:\n",
    "                # Handle empty titles (if any)\n",
    "                embeddings_list.append(torch.zeros(embedding_dim).cpu().numpy())  # Zero vector for empty titles\n",
    "    \n",
    "    return embeddings_list\n",
    "\n",
    "# Generate embeddings for all the tokenized titles\n",
    "embeddings = get_embeddings_for_titles(tokenized_titles, mFoo)\n",
    "\n",
    "# Step 6: Store the embeddings\n",
    "# Convert to a DataFrame for easier saving/processing later\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "# Save the embeddings to a file (optional)\n",
    "embeddings_df.to_csv('title_embeddings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\two_towers\\wandb\\run-20241022_204004-8n4k5vdx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/twotower_training/runs/8n4k5vdx' target=\"_blank\">fine_tuning_skipgram/final/10ep</a></strong> to <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/twotower_training/runs/8n4k5vdx' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training/runs/8n4k5vdx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_26928\\3823745590.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_save_path)\n",
      "Epoch 1/10: 100%|██████████| 1287/1287 [07:26<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed. Loss: 0.6824356913566589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1287/1287 [07:28<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 completed. Loss: 0.6674662232398987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1287/1287 [07:18<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 completed. Loss: 0.5962892770767212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1287/1287 [07:22<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 completed. Loss: 0.6967517137527466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1287/1287 [07:25<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 completed. Loss: 0.661889910697937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1287/1287 [07:19<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 completed. Loss: 0.5973553657531738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1287/1287 [07:21<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 completed. Loss: 0.5778597593307495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1287/1287 [07:25<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 completed. Loss: 0.6206641793251038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1287/1287 [07:27<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 completed. Loss: 0.5360973477363586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1287/1287 [07:57<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 completed. Loss: 0.47312402725219727\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence  # For padding sequences\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import wandb  # Import Weights and Biases\n",
    "\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"twotower_training\", entity=\"omareweis123\", name='fine_tuning_skipgram/final/10ep')\n",
    "\n",
    "# Load the saved SkipGram model\n",
    "model_save_path = \"finetuned_skipgram_model.pth\"\n",
    "checkpoint = torch.load(model_save_path)\n",
    "\n",
    "# Initialize the SkipGram model\n",
    "skipgram_model = SkipGramFoo(121598, 64, 2).to(device)  # Ensure to send the model to the correct device\n",
    "skipgram_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "skipgram_model.eval()  # Set the model to evaluation mode if you're not training it again\n",
    "\n",
    "# Tower RNN Models\n",
    "tower_one = TowerOneRNN().to(device)\n",
    "tower_two = TowerTwoRNN().to(device)\n",
    "\n",
    "# Triplet margin loss with cosine distance\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=1.0\n",
    ")\n",
    "\n",
    "# Define optimizer for the tower models\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=0.001)\n",
    "\n",
    "# Define query batch size\n",
    "query_batch_size = 64  # Number of queries to process in a batch\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 10  # Example\n",
    "\n",
    "# Track model and hyperparameters in W&B\n",
    "wandb.watch([tower_one, tower_two], log=\"all\")  # Log gradients and model weights\n",
    "\n",
    "# Training loop with batched queries\n",
    "for epoch in range(num_epochs):\n",
    "    # Group by 'query' to handle variable number of positives and negatives per query\n",
    "    query_groups = list(results.groupby('query'))\n",
    "\n",
    "    # Iterate through the dataset in batches of 'query_batch_size' queries\n",
    "    for q_batch_start in tqdm(range(0, len(query_groups), query_batch_size), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        query_batch = query_groups[q_batch_start:q_batch_start + query_batch_size]\n",
    "\n",
    "        all_anchor_embeddings = []\n",
    "        all_positive_embeddings = []\n",
    "        all_negative_embeddings = []\n",
    "\n",
    "        # Process each query group in the current batch\n",
    "        for query, group in query_batch:\n",
    "            # Tokenize the queries, passage_text, and negative_sample using your updated_vocab or reverse_vocab\n",
    "            query_tokens = tokenize_titles(group['query'].tolist(), reverse_vocab)\n",
    "            positive_tokens = tokenize_titles(group['passage_text'].tolist(), reverse_vocab)\n",
    "            negative_tokens = tokenize_titles(group['negative_sample'].tolist(), reverse_vocab)\n",
    "\n",
    "            # Get embeddings for each group\n",
    "            anchor_embeddings = get_embeddings_for_titles(query_tokens, skipgram_model)\n",
    "            positive_embeddings = get_embeddings_for_titles(positive_tokens, skipgram_model)\n",
    "            negative_embeddings = get_embeddings_for_titles(negative_tokens, skipgram_model)\n",
    "\n",
    "            # Append the embeddings to the lists\n",
    "            all_anchor_embeddings.append(torch.tensor(anchor_embeddings))\n",
    "            all_positive_embeddings.append(torch.tensor(positive_embeddings))\n",
    "            all_negative_embeddings.append(torch.tensor(negative_embeddings))\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        anchor_batch = pad_sequence(all_anchor_embeddings, batch_first=True).to(device)\n",
    "        positive_batch = pad_sequence(all_positive_embeddings, batch_first=True).to(device)\n",
    "        negative_batch = pad_sequence(all_negative_embeddings, batch_first=True).to(device)\n",
    "\n",
    "        # Forward pass through the two towers (anchor -> TowerOne, positive/negative -> TowerTwo)\n",
    "        anchor_output = tower_one(anchor_batch)  # anchor_batch is now 3D (batch_size, sequence_length, embedding_size)\n",
    "        positive_output = tower_two(positive_batch)\n",
    "        negative_output = tower_two(negative_batch)\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss to W&B\n",
    "        wandb.log({\"epoch\": epoch + 1, \"triplet_loss\": triplet_loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {triplet_loss.item()}\")\n",
    "\n",
    "    # Optionally, save a model checkpoint after every epoch and log it to W&B\n",
    "    torch.save({\n",
    "        'model_state_dict': skipgram_model.state_dict(),\n",
    "        'tower_one_state_dict': tower_one.state_dict(),\n",
    "        'tower_two_state_dict': tower_two.state_dict(),\n",
    "    }, f\"model_checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    wandb.finish\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_26928\\3043405766.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_save_path)\n",
      "Epoch 1/1000:   2%|▏         | 1372/82326 [00:15<15:18, 88.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Create tensors for inputs, ensuring they're on the correct device\u001b[39;00m\n\u001b[0;32m     40\u001b[0m anchor_embeddings \u001b[38;5;241m=\u001b[39m get_embeddings_for_titles(query_tokens, skipgram_model)\n\u001b[1;32m---> 41\u001b[0m positive_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings_for_titles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipgram_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m negative_embeddings \u001b[38;5;241m=\u001b[39m get_embeddings_for_titles(negative_tokens, skipgram_model)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Move embeddings to device and add sequence dimension\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 98\u001b[0m, in \u001b[0;36mget_embeddings_for_titles\u001b[1;34m(tokenized_titles, model)\u001b[0m\n\u001b[0;32m     95\u001b[0m token_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(tokens)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Get the embeddings for each token in the title\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m(token_tensor)  \u001b[38;5;66;03m# Shape: [num_tokens, embedding_dim]\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Average the token embeddings to get a single vector for the entire title\u001b[39;00m\n\u001b[0;32m    101\u001b[0m title_embedding \u001b[38;5;241m=\u001b[39m token_embeddings\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: [embedding_dim]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1716\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1718\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Load the saved SkipGram model\n",
    "model_save_path = \"finetuned_skipgram_model.pth\"\n",
    "checkpoint = torch.load(model_save_path)\n",
    "\n",
    "# Initialize the SkipGram model\n",
    "skipgram_model = SkipGramFoo(121598, 64, 2).to(device)  # Ensure to send the model to the correct device\n",
    "skipgram_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "skipgram_model.eval()  # Set the model to evaluation mode if you're not training it again\n",
    "\n",
    "# Tower RNN Models\n",
    "tower_one = TowerOneRNN().to(device)\n",
    "tower_two = TowerTwoRNN().to(device)\n",
    "\n",
    "# Triplet margin loss with cosine distance\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=1.0\n",
    ")\n",
    "\n",
    "# Define optimizer for the tower models\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=0.001)\n",
    "\n",
    "# Training with query-based batching\n",
    "for epoch in range(num_epochs):\n",
    "    # Group by 'query' to handle variable number of positives and negatives per query\n",
    "    query_groups = results.groupby('query')\n",
    "    \n",
    "    for query, group in tqdm(query_groups, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        # Tokenize the queries, passage_text, and negative_sample using your updated_vocab or reverse_vocab\n",
    "        query_tokens = tokenize_titles(group['query'].tolist(), reverse_vocab)\n",
    "        positive_tokens = tokenize_titles(group['passage_text'].tolist(), reverse_vocab)\n",
    "        negative_tokens = tokenize_titles(group['negative_sample'].tolist(), reverse_vocab)\n",
    "\n",
    "        # Create tensors for inputs, ensuring they're on the correct device\n",
    "        anchor_embeddings = get_embeddings_for_titles(query_tokens, skipgram_model)\n",
    "        positive_embeddings = get_embeddings_for_titles(positive_tokens, skipgram_model)\n",
    "        negative_embeddings = get_embeddings_for_titles(negative_tokens, skipgram_model)\n",
    "\n",
    "        # Move embeddings to device and add sequence dimension\n",
    "        anchor_embeddings = torch.tensor(anchor_embeddings).unsqueeze(1).to(device)  # (batch_size, 1, embedding_size)\n",
    "        positive_embeddings = torch.tensor(positive_embeddings).unsqueeze(1).to(device)  # Same for positive\n",
    "        negative_embeddings = torch.tensor(negative_embeddings).unsqueeze(1).to(device)  # Same for negative\n",
    "\n",
    "        # Forward pass through the two towers (anchor -> TowerOne, positive/negative -> TowerTwo)\n",
    "        anchor_output = tower_one(anchor_embeddings)\n",
    "        positive_output = tower_two(positive_embeddings)\n",
    "        negative_output = tower_two(negative_embeddings)\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {triplet_loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_26928\\418175315.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_save_path)\n",
      "Epoch 1/1000:   0%|          | 0/82326 [00:00<?, ?it/s]C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_26928\\418175315.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  anchor_embeddings = torch.tensor(anchor_embeddings).to(device)\n",
      "Epoch 1/1000:   0%|          | 0/82326 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m negative_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(negative_embeddings)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Forward pass through the two towers (anchor -> TowerOne, positive/negative -> TowerTwo)\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m anchor_output \u001b[38;5;241m=\u001b[39m \u001b[43mtower_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m positive_output \u001b[38;5;241m=\u001b[39m tower_two(positive_embeddings)\n\u001b[0;32m     52\u001b[0m negative_output \u001b[38;5;241m=\u001b[39m tower_two(negative_embeddings)\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 19\u001b[0m, in \u001b[0;36mTowerOneRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# The LSTM outputs sequences; get the output from the last time step\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Keep the output for the last time step\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Pass through the fully connected layer\u001b[39;00m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Load the saved SkipGram model\n",
    "model_save_path = \"finetuned_skipgram_model.pth\"\n",
    "checkpoint = torch.load(model_save_path)\n",
    "\n",
    "# Initialize the SkipGram model\n",
    "skipgram_model = SkipGramFoo(121598, 64, 2).to(device)  # Ensure to send the model to the correct device\n",
    "skipgram_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "skipgram_model.eval()  # Set the model to evaluation mode if you're not training it again\n",
    "\n",
    "# Tower RNN Models\n",
    "tower_one = TowerOneRNN().to(device)\n",
    "tower_two = TowerTwoRNN().to(device)\n",
    "\n",
    "# Triplet margin loss with cosine distance\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=1.0\n",
    ")\n",
    "\n",
    "# Define optimizer for the tower models\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=0.001)\n",
    "\n",
    "# Training with query-based batching\n",
    "for epoch in range(num_epochs):\n",
    "    # Group by 'query' to handle variable number of positives and negatives per query\n",
    "    query_groups = results.groupby('query')\n",
    "    \n",
    "    for query, group in tqdm(query_groups, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        # Tokenize the queries, passage_text, and negative_sample using your updated_vocab or reverse_vocab\n",
    "        query_tokens = tokenize_titles(group['query'].tolist(), reverse_vocab)\n",
    "        positive_tokens = tokenize_titles(group['passage_text'].tolist(), reverse_vocab)\n",
    "        negative_tokens = tokenize_titles(group['negative_sample'].tolist(), reverse_vocab)\n",
    "\n",
    "        # Create tensors for inputs, ensuring they're on the correct device\n",
    "        anchor_embeddings = get_embeddings_for_titles(query_tokens, skipgram_model)\n",
    "        positive_embeddings = get_embeddings_for_titles(positive_tokens, skipgram_model)\n",
    "        negative_embeddings = get_embeddings_for_titles(negative_tokens, skipgram_model)\n",
    "\n",
    "        # Move embeddings to device\n",
    "        anchor_embeddings = torch.tensor(anchor_embeddings).to(device)\n",
    "        positive_embeddings = torch.tensor(positive_embeddings).to(device)\n",
    "        negative_embeddings = torch.tensor(negative_embeddings).to(device)\n",
    "\n",
    "        # Forward pass through the two towers (anchor -> TowerOne, positive/negative -> TowerTwo)\n",
    "        anchor_output = tower_one(anchor_embeddings)\n",
    "        positive_output = tower_two(positive_embeddings)\n",
    "        negative_output = tower_two(negative_embeddings)\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {triplet_loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_26928\\2370165924.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_save_path)\n",
      "Epoch 1/1000:   0%|          | 0/82326 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SkipGramFoo.forward() missing 2 required positional arguments: 'trgs' and 'rand'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m query_groups \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query, group \u001b[38;5;129;01min\u001b[39;00m tqdm(query_groups, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Anchor: query embeddings\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     anchor_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mskipgram_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Positive samples: embedding of passage_text\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     positive_embeddings \u001b[38;5;241m=\u001b[39m skipgram_model(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassage_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: SkipGramFoo.forward() missing 2 required positional arguments: 'trgs' and 'rand'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Load the saved SkipGram model\n",
    "model_save_path = \"finetuned_skipgram_model.pth\"\n",
    "checkpoint = torch.load(model_save_path)\n",
    "\n",
    "# Initialize the SkipGram model\n",
    "skipgram_model = SkipGramFoo(121598, 64, 2)\n",
    "skipgram_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Tower RNN Models\n",
    "tower_one = TowerOneRNN()\n",
    "tower_two = TowerTwoRNN()\n",
    "\n",
    "# Triplet margin loss with cosine distance\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=1.0\n",
    ")\n",
    "\n",
    "# Define optimizer for the tower models\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=0.001)\n",
    "\n",
    "# Training with query-based batching\n",
    "for epoch in range(num_epochs):\n",
    "    # Group by 'query' to handle variable number of positives and negatives per query\n",
    "    query_groups = results.groupby('query')\n",
    "    \n",
    "    for query, group in tqdm(query_groups, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        # Anchor: query embeddings\n",
    "        anchor_embeddings = skipgram_model(group['query'].tolist())\n",
    "        \n",
    "        # Positive samples: embedding of passage_text\n",
    "        positive_embeddings = skipgram_model(group['passage_text'].tolist())\n",
    "        \n",
    "        # Negative samples: embedding of negative_sample\n",
    "        negative_embeddings = skipgram_model(group['negative_sample'].tolist())\n",
    "        \n",
    "        # Forward pass through the two towers (anchor -> TowerOne, positive/negative -> TowerTwo)\n",
    "        anchor_output = tower_one(anchor_embeddings)\n",
    "        positive_output = tower_two(positive_embeddings)\n",
    "        negative_output = tower_two(negative_embeddings)\n",
    "        \n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {triplet_loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
