{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a matrix of embedders I can use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the df \n",
    "results_df = pd.read_csv('results_negative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in dictionary \n",
    "# Loading the vocab_dict again\n",
    "import pickle\n",
    "with open(\"updated_vocab_dict.pkl\", \"rb\") as f:\n",
    "    updated_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.5</th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>query</th>\n",
       "      <th>passage_text</th>\n",
       "      <th>negative_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>since the rba outstanding reputation been affe...</td>\n",
       "      <td>unaffiliated the majority religion in chile in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>the reserve bank of rba came into being on as ...</td>\n",
       "      <td>days on average range days for the infectious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>rba with the us regional partner of the by pr ...</td>\n",
       "      <td>citrus tree that does not get enough water bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>the inner of atomizer are surprisingly simple ...</td>\n",
       "      <td>or are herbal that promote greater density and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>based accountability also known as rba is way ...</td>\n",
       "      <td>find by star from the big dipper if you re fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676183</th>\n",
       "      <td>676183</td>\n",
       "      <td>676183</td>\n",
       "      <td>676183</td>\n",
       "      <td>676183</td>\n",
       "      <td>676188</td>\n",
       "      <td>676188</td>\n",
       "      <td>what is polarity index definition</td>\n",
       "      <td>water is an example of polar molecule since it...</td>\n",
       "      <td>size relative to tea cup the porcupine is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676184</th>\n",
       "      <td>676184</td>\n",
       "      <td>676184</td>\n",
       "      <td>676184</td>\n",
       "      <td>676184</td>\n",
       "      <td>676189</td>\n",
       "      <td>676189</td>\n",
       "      <td>what is polarity index definition</td>\n",
       "      <td>supplement can either be polar or nonpolar dep...</td>\n",
       "      <td>average air hostess the average salary for air...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676185</th>\n",
       "      <td>676185</td>\n",
       "      <td>676185</td>\n",
       "      <td>676185</td>\n",
       "      <td>676185</td>\n",
       "      <td>676190</td>\n",
       "      <td>676190</td>\n",
       "      <td>what is polarity index definition</td>\n",
       "      <td>full definition of polarity the quality or con...</td>\n",
       "      <td>one of ranging from to hip hop in june or each...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676186</th>\n",
       "      <td>676186</td>\n",
       "      <td>676186</td>\n",
       "      <td>676186</td>\n",
       "      <td>676186</td>\n",
       "      <td>676191</td>\n",
       "      <td>676191</td>\n",
       "      <td>what is polarity index definition</td>\n",
       "      <td>part of the smart grid glossary also see bipol...</td>\n",
       "      <td>days of hot weather in the temperature at into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676187</th>\n",
       "      <td>676187</td>\n",
       "      <td>676187</td>\n",
       "      <td>676187</td>\n",
       "      <td>676187</td>\n",
       "      <td>676192</td>\n",
       "      <td>676192</td>\n",
       "      <td>what is polarity index definition</td>\n",
       "      <td>polarity the condition of general physics the ...</td>\n",
       "      <td>the salary for someone with the job title cert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676188 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.5  Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  \\\n",
       "0                  0             0             0             0             0   \n",
       "1                  1             1             1             1             1   \n",
       "2                  2             2             2             2             2   \n",
       "3                  3             3             3             3             3   \n",
       "4                  4             4             4             4             4   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "676183        676183        676183        676183        676183        676188   \n",
       "676184        676184        676184        676184        676184        676189   \n",
       "676185        676185        676185        676185        676185        676190   \n",
       "676186        676186        676186        676186        676186        676191   \n",
       "676187        676187        676187        676187        676187        676192   \n",
       "\n",
       "        Unnamed: 0                              query  \\\n",
       "0                0                        what is rba   \n",
       "1                1                        what is rba   \n",
       "2                2                        what is rba   \n",
       "3                3                        what is rba   \n",
       "4                4                        what is rba   \n",
       "...            ...                                ...   \n",
       "676183      676188  what is polarity index definition   \n",
       "676184      676189  what is polarity index definition   \n",
       "676185      676190  what is polarity index definition   \n",
       "676186      676191  what is polarity index definition   \n",
       "676187      676192  what is polarity index definition   \n",
       "\n",
       "                                             passage_text  \\\n",
       "0       since the rba outstanding reputation been affe...   \n",
       "1       the reserve bank of rba came into being on as ...   \n",
       "2       rba with the us regional partner of the by pr ...   \n",
       "3       the inner of atomizer are surprisingly simple ...   \n",
       "4       based accountability also known as rba is way ...   \n",
       "...                                                   ...   \n",
       "676183  water is an example of polar molecule since it...   \n",
       "676184  supplement can either be polar or nonpolar dep...   \n",
       "676185  full definition of polarity the quality or con...   \n",
       "676186  part of the smart grid glossary also see bipol...   \n",
       "676187  polarity the condition of general physics the ...   \n",
       "\n",
       "                                          negative_sample  \n",
       "0       unaffiliated the majority religion in chile in...  \n",
       "1       days on average range days for the infectious ...  \n",
       "2       citrus tree that does not get enough water bec...  \n",
       "3       or are herbal that promote greater density and...  \n",
       "4       find by star from the big dipper if you re fam...  \n",
       "...                                                   ...  \n",
       "676183  size relative to tea cup the porcupine is the ...  \n",
       "676184  average air hostess the average salary for air...  \n",
       "676185  one of ranging from to hip hop in june or each...  \n",
       "676186  days of hot weather in the temperature at into...  \n",
       "676187  the salary for someone with the job title cert...  \n",
       "\n",
       "[676188 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np  # Import the tqdm library for progress bars\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to create a reverse vocabulary from the updated vocabulary\n",
    "def create_reverse_vocab(vocab):\n",
    "    return {word: index for index, word in vocab.items()}\n",
    "\n",
    "# Load the reverse vocabulary\n",
    "reverse_vocab = create_reverse_vocab(updated_vocab)\n",
    "\n",
    "# Function to tokenize a list of texts using the reverse vocabulary\n",
    "def tokenize_texts(texts, reverse_vocab):\n",
    "    tokens_list = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.lower().split()  # Convert the text to lowercase to match training preprocessing\n",
    "        \n",
    "        tokenized = []\n",
    "        for word in words:\n",
    "            if word in reverse_vocab:\n",
    "                tokenized.append(reverse_vocab[word])  # Get the index from reverse_vocab\n",
    "            # No else clause needed; unknown words are simply skipped\n",
    "        \n",
    "        tokens_list.append(tokenized)\n",
    "    \n",
    "    return tokens_list\n",
    "\n",
    "# Step 1: Shuffle the DataFrame row-wise\n",
    "shuffled_df = results_df.sample(frac=1).reset_index(drop=True)  # Shuffle the dataframe\n",
    "\n",
    "# Step 2: Tokenize the query, passage_text, and negative_samples columns\n",
    "tokenized_queries = tokenize_texts(shuffled_df['query'].tolist(), reverse_vocab)\n",
    "tokenized_passages = tokenize_texts(shuffled_df['passage_text'].tolist(), reverse_vocab)\n",
    "tokenized_negatives = tokenize_texts(shuffled_df['negative_sample'].tolist(), reverse_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_27728\\1276856253.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mFoo.load_state_dict(torch.load(model_path), strict=False)\n",
      "Generating embeddings: 100%|██████████| 676188/676188 [02:02<00:00, 5536.87text/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Define the SkipGramFoo model\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "    def __init__(self, voc, emb, ctx):\n",
    "        super().__init__()\n",
    "        self.ctx = ctx\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "        self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inpt, trgs, rand):\n",
    "        emb = self.emb(inpt)\n",
    "        batch_size = inpt.size(0)\n",
    "        rand = rand[:batch_size]\n",
    "        \n",
    "        ctx = self.ffw.weight[trgs.to(inpt.device)]\n",
    "        rnd = self.ffw.weight[rand.to(inpt.device)]\n",
    "        \n",
    "        out = torch.bmm(ctx.view(batch_size, 1, -1), emb.unsqueeze(2)).squeeze(2)\n",
    "        rnd = torch.bmm(rnd.view(batch_size, 1, -1), emb.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "        rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "        pst = -out.log().mean()\n",
    "        ngt = -(1 - rnd).log().mean()\n",
    "        \n",
    "        return pst + ngt\n",
    "\n",
    "# Load the model\n",
    "embedding_dim = 64\n",
    "model_path = \"finetuned_skipgram_model.pth\"\n",
    "mFoo = SkipGramFoo(len(updated_vocab), embedding_dim, 2).to(device)\n",
    "mFoo.load_state_dict(torch.load(model_path), strict=False)\n",
    "mFoo.eval()\n",
    "\n",
    "# Step 4: Generate embeddings for tokenized texts (query, passage, negative samples) with a progress bar\n",
    "def get_embeddings_for_tokens(tokenized_list, model):\n",
    "    embeddings_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tokens in tqdm(tokenized_list, desc=\"Generating embeddings\", unit=\"text\"):  # Add progress bar\n",
    "            if len(tokens) > 0:\n",
    "                token_tensor = torch.LongTensor(tokens).to(device)\n",
    "                token_embeddings = model.emb(token_tensor)  # Shape: [num_tokens, embedding_dim]\n",
    "                embeddings_list.append(token_embeddings.cpu().numpy())  # Keep the full sequence embeddings\n",
    "            else:\n",
    "                embeddings_list.append(torch.zeros((1, embedding_dim)).cpu().numpy())  # Zero vector for empty sequences\n",
    "    \n",
    "    return embeddings_list\n",
    "\n",
    "query_embeddings = get_embeddings_for_tokens(tokenized_queries, mFoo)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the list directly as a pickle file\n",
    "with open('query_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(query_embeddings, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 676188/676188 [02:14<00:00, 5041.55text/s]\n"
     ]
    }
   ],
   "source": [
    "passage_embeddings = get_embeddings_for_tokens(tokenized_passages, mFoo)\n",
    "with open('passage_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(query_embeddings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 676188/676188 [02:13<00:00, 5074.11text/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "negative_embeddings = get_embeddings_for_tokens(tokenized_negatives, mFoo)\n",
    "with open('negative_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(query_embeddings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.5                                                       1\n",
       "Unnamed: 0.4                                                       1\n",
       "Unnamed: 0.3                                                       1\n",
       "Unnamed: 0.2                                                       1\n",
       "Unnamed: 0.1                                                       1\n",
       "Unnamed: 0                                                         1\n",
       "query                                                    what is rba\n",
       "passage_text       the reserve bank of rba came into being on as ...\n",
       "negative_sample    days on average range days for the infectious ...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the first negative embedding: 4\n"
     ]
    }
   ],
   "source": [
    "# indexing embeddings \n",
    "first_negative_embedding = query_embeddings[1]  # Get the first embedding\n",
    "\n",
    "# Check the length of the first embedding\n",
    "length_of_first_embedding = len(first_negative_embedding)\n",
    "\n",
    "print(\"Length of the first negative embedding:\", length_of_first_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence  \n",
    "from tqdm import tqdm  \n",
    "import wandb  \n",
    "\n",
    "\n",
    "class TowerOneRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerOneRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=64, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass input through the RNN layer\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Unpack the output and get the last time step\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x = x[torch.arange(x.size(0)), lengths - 1]  # Get the last valid time step\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TowerTwoRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerTwoRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=64, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass input through the RNN layer\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Unpack the output and get the last time step\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x = x[torch.arange(x.size(0)), lengths - 1]  # Get the last valid time step\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('query_embeddings.pkl', 'rb') as f:\n",
    "    query_embeddings = pickle.load(f)\n",
    "\n",
    "with open('passage_embeddings.pkl', 'rb') as f:\n",
    "    passage_embeddings = pickle.load(f)\n",
    "\n",
    "with open('negative_embeddings.pkl', 'rb') as f:\n",
    "    negative_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676188"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(passage_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:owvn4lpz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_triplet_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_triplet_loss</td><td>1</td></tr><tr><td>epoch</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine_tuning_skipgram/packed_no_paddingfixed</strong> at: <a href='https://wandb.ai/omareweis123/twotower_training/runs/owvn4lpz' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training/runs/owvn4lpz</a><br/> View project at: <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_131157-owvn4lpz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:owvn4lpz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\two_towers\\wandb\\run-20241025_131248-vad2sgjp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/twotower_training/runs/vad2sgjp' target=\"_blank\">fine_tuning_skipgram/packed_new_skipgram_betterpre</a></strong> to <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/twotower_training/runs/vad2sgjp' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training/runs/vad2sgjp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 5283/5283 [02:24<00:00, 36.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Loss: 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>triplet_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>triplet_loss</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine_tuning_skipgram/packed_new_skipgram_betterpre</strong> at: <a href='https://wandb.ai/omareweis123/twotower_training/runs/vad2sgjp' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training/runs/vad2sgjp</a><br/> View project at: <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_131248-vad2sgjp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   0%|          | 0/5283 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Log the loss to W&B\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtriplet_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriplet_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed. Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtriplet_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\wandb\\sdk\\lib\\preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "wandb.init(project=\"twotower_training\", entity=\"omareweis123\", name='fine_tuning_skipgram/packed_new_skipgram_betterpre')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Tower RNN Models\n",
    "tower_one = TowerOneRNN().to(device)\n",
    "tower_two = TowerTwoRNN().to(device)\n",
    "\n",
    "# Triplet margin loss with cosine distance\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=1,\n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "# Define optimizer for the tower models\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=0.001)\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 10 # Example\n",
    "\n",
    "# Track model and hyperparameters in W&B\n",
    "wandb.watch([tower_one, tower_two], log=\"all\")  # Log gradients and model weights\n",
    "\n",
    "# Total samples and batch size\n",
    "total_samples = len(query_embeddings)  # Assuming all arrays have the same length\n",
    "batch_size = 128  # Number of embeddings to process in a batch\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the indices for the embeddings\n",
    "    indices = np.random.permutation(total_samples)  # Shuffle indices\n",
    "\n",
    "    for i in tqdm(range(0, total_samples, batch_size), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        # Select the batch of embeddings by index\n",
    "        batch_indices = indices[i:min(i + batch_size, total_samples)]\n",
    "        anchor_batch = [query_embeddings[idx] for idx in batch_indices]\n",
    "        positive_batch = [passage_embeddings[idx] for idx in batch_indices]\n",
    "        negative_batch = [negative_embeddings[idx] for idx in batch_indices]\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        anchor_batch = pad_sequence([torch.tensor(e) for e in anchor_batch], batch_first=True).to(device)\n",
    "        positive_batch = pad_sequence([torch.tensor(e) for e in positive_batch], batch_first=True).to(device)\n",
    "        negative_batch = pad_sequence([torch.tensor(e) for e in negative_batch], batch_first=True).to(device)\n",
    "\n",
    "        # Get lengths of the original sequences\n",
    "        anchor_lengths = torch.tensor([len(e) for e in [query_embeddings[idx] for idx in batch_indices]]).cpu()\n",
    "        positive_lengths = torch.tensor([len(e) for e in [passage_embeddings[idx] for idx in batch_indices]]).cpu()\n",
    "        negative_lengths = torch.tensor([len(e) for e in [negative_embeddings[idx] for idx in batch_indices]]).cpu()\n",
    "\n",
    "        # Forward pass through the two towers\n",
    "        anchor_output = tower_one(anchor_batch, anchor_lengths)\n",
    "        positive_output = tower_two(positive_batch, positive_lengths)\n",
    "        negative_output = tower_two(negative_batch, negative_lengths)\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss to W&B\n",
    "        wandb.log({\"epoch\": epoch + 1, \"triplet_loss\": triplet_loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {triplet_loss.item()}\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\two_towers\\wandb\\run-20241025_152624-xr0bxdr0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/twotower_training/runs/xr0bxdr0' target=\"_blank\">fine_tuning_skipgram/packed_no_shuffling </a></strong> to <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/twotower_training/runs/xr0bxdr0' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training/runs/xr0bxdr0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 5283/5283 [03:09<00:00, 27.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 completed. Loss: 1.038094401359558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 5283/5283 [03:09<00:00, 27.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 completed. Loss: 1.0059922933578491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50:  48%|████▊     | 2544/5283 [01:31<01:38, 27.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 67\u001b[0m \u001b[43mtriplet_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Log the loss to W&B\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"twotower_training\", entity=\"omareweis123\", name='fine_tuning_skipgram/packed_no_shuffling ')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tower RNN Models\n",
    "tower_one = TowerOneRNN().to(device)\n",
    "tower_two = TowerTwoRNN().to(device)\n",
    "\n",
    "# Triplet margin loss with cosine distance\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=1.0,\n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "# Define optimizer for the tower models\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=0.001)\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 50  # Example\n",
    "\n",
    "# Track model and hyperparameters in W&B\n",
    "wandb.watch([tower_one, tower_two], log=\"all\")  # Log gradients and model weights\n",
    "\n",
    "# Total samples and batch size\n",
    "total_samples = len(query_embeddings)  # Assuming all arrays have the same length\n",
    "batch_size = 128  # Number of embeddings to process in a batch\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate through the dataset without shuffling, in batches\n",
    "    for i in tqdm(range(0, total_samples, batch_size), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        # Select the batch of embeddings by index\n",
    "        batch_indices = range(i, min(i + batch_size, total_samples))\n",
    "        anchor_batch = [query_embeddings[idx] for idx in batch_indices]\n",
    "        positive_batch = [passage_embeddings[idx] for idx in batch_indices]\n",
    "        negative_batch = [negative_embeddings[idx] for idx in batch_indices]\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        anchor_batch = pad_sequence([torch.tensor(e) for e in anchor_batch], batch_first=True).to(device)\n",
    "        positive_batch = pad_sequence([torch.tensor(e) for e in positive_batch], batch_first=True).to(device)\n",
    "        negative_batch = pad_sequence([torch.tensor(e) for e in negative_batch], batch_first=True).to(device)\n",
    "\n",
    "        # Get lengths of the original sequences\n",
    "        anchor_lengths = torch.tensor([len(seq) for seq in anchor_batch]).cpu()\n",
    "        positive_lengths = torch.tensor([len(seq) for seq in positive_batch]).cpu()\n",
    "        negative_lengths = torch.tensor([len(seq) for seq in negative_batch]).cpu()\n",
    "\n",
    "        # Forward pass through the two towers\n",
    "        anchor_output = tower_one(anchor_batch, anchor_lengths)\n",
    "        positive_output = tower_two(positive_batch, positive_lengths)\n",
    "        negative_output = tower_two(negative_batch, negative_lengths)\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss to W&B\n",
    "        wandb.log({\"epoch\": epoch + 1, \"triplet_loss\": triplet_loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {triplet_loss.item()}\")\n",
    "\n",
    "# Finish W&B logging after all epochs\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\two_towers\\wandb\\run-20241024_113338-hinmiil1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/twotower_training/runs/hinmiil1' target=\"_blank\">fine_tuning_skipgram/packed_no_shuffling</a></strong> to <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/twotower_training/runs/hinmiil1' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training/runs/hinmiil1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 10566/10566 [05:22<00:00, 32.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Loss: 1.0010591745376587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 10566/10566 [05:20<00:00, 32.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed. Loss: 0.996854841709137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 10566/10566 [05:21<00:00, 32.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed. Loss: 1.00316321849823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  87%|████████▋ | 9174/10566 [04:38<00:42, 32.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m anchor_output \u001b[38;5;241m=\u001b[39m tower_one(anchor_batch, anchor_lengths)\n\u001b[0;32m     65\u001b[0m positive_output \u001b[38;5;241m=\u001b[39m tower_two(positive_batch, positive_lengths)\n\u001b[1;32m---> 66\u001b[0m negative_output \u001b[38;5;241m=\u001b[39m \u001b[43mtower_two\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate triplet loss\u001b[39;00m\n\u001b[0;32m     69\u001b[0m triplet_loss \u001b[38;5;241m=\u001b[39m triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1608\u001b[0m     ):\n\u001b[0;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mTowerTwoRNN.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpack_padded_sequence(x, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Pass input through the RNN layer\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Unpack the output and get the last time step\u001b[39;00m\n\u001b[0;32m     45\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_packed_sequence(x, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omare\\anaconda3\\envs\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:601\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 601\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    606\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m    607\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# Initialize Weights and Biases (W&B)\n",
    "wandb.init(project=\"twotower_training\", entity=\"omareweis123\", name='fine_tuning_skipgram/packed_no_shuffling')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tower RNN Models\n",
    "tower_one = TowerOneRNN().to(device)\n",
    "tower_two = TowerTwoRNN().to(device)\n",
    "\n",
    "# Triplet margin loss with cosine distance\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=1.0,\n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "# Define optimizer for the tower models\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=0.001)\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 5  # Example\n",
    "\n",
    "# Track model and hyperparameters in W&B\n",
    "wandb.watch([tower_one, tower_two], log=\"all\")  # Log gradients and model weights\n",
    "\n",
    "# Ensure the embeddings are converted to lists (if needed)\n",
    "query_embeddings = [np.array(embedding) for embedding in query_embeddings]\n",
    "passage_embeddings = [np.array(embedding) for embedding in passage_embeddings]\n",
    "negative_embeddings = [np.array(embedding) for embedding in negative_embeddings]\n",
    "\n",
    "# Total samples and batch size\n",
    "total_samples = len(query_embeddings)  # Assuming all arrays have the same length\n",
    "batch_size = 64  # Number of embeddings to process in a batch\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate through the dataset without shuffling, in batches\n",
    "    for i in tqdm(range(0, total_samples, batch_size), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        # Select the batch of embeddings by index\n",
    "        batch_indices = range(i, min(i + batch_size, total_samples))\n",
    "        anchor_batch = [query_embeddings[idx] for idx in batch_indices]\n",
    "        positive_batch = [passage_embeddings[idx] for idx in batch_indices]\n",
    "        negative_batch = [negative_embeddings[idx] for idx in batch_indices]\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        anchor_batch = pad_sequence([torch.tensor(e) for e in anchor_batch], batch_first=True).to(device)\n",
    "        positive_batch = pad_sequence([torch.tensor(e) for e in positive_batch], batch_first=True).to(device)\n",
    "        negative_batch = pad_sequence([torch.tensor(e) for e in negative_batch], batch_first=True).to(device)\n",
    "\n",
    "        # Get lengths of the original sequences\n",
    "        anchor_lengths = torch.tensor([len(seq) for seq in anchor_batch]).cpu()\n",
    "        positive_lengths = torch.tensor([len(seq) for seq in positive_batch]).cpu()\n",
    "        negative_lengths = torch.tensor([len(seq) for seq in negative_batch]).cpu()\n",
    "\n",
    "        # Forward pass through the two towers\n",
    "        anchor_output = tower_one(anchor_batch, anchor_lengths)\n",
    "        positive_output = tower_two(positive_batch, positive_lengths)\n",
    "        negative_output = tower_two(negative_batch, negative_lengths)\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss to W&B\n",
    "        wandb.log({\"epoch\": epoch + 1, \"triplet_loss\": triplet_loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {triplet_loss.item()}\")\n",
    "\n",
    "# Finish W&B logging at the end of the training\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
