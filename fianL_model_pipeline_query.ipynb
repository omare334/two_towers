{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model based on queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in dictionary \n",
    "# Loading the vocab_dict again\n",
    "import pickle\n",
    "with open(\"updated_vocab_dict.pkl\", \"rb\") as f:\n",
    "    updated_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np  # Import the tqdm library for progress bars\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to create a reverse vocabulary from the updated vocabulary\n",
    "def create_reverse_vocab(vocab):\n",
    "    return {word: index for index, word in vocab.items()}\n",
    "\n",
    "# Load the reverse vocabulary\n",
    "reverse_vocab = create_reverse_vocab(updated_vocab)\n",
    "\n",
    "# Tokenize the titles using the reverse vocabulary\n",
    "def tokenize_titles(titles, reverse_vocab):\n",
    "    tokens = []\n",
    "    \n",
    "    for title in titles:\n",
    "        words = title.lower().split()  # Convert the title to lowercase to match training preprocessing\n",
    "        \n",
    "        tokenized = []\n",
    "        for word in words:\n",
    "            if word in reverse_vocab:\n",
    "                tokenized.append(reverse_vocab[word])  # Get the index from reverse_vocab\n",
    "            # No else clause needed; unknown words are simply skipped\n",
    "        \n",
    "        tokens.append(tokenized)\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_21960\\3985565947.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mFoo.load_state_dict(torch.load(model_path), strict=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Define the SkipGramFoo model\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "    def __init__(self, voc, emb, ctx):\n",
    "        super().__init__()\n",
    "        self.ctx = ctx\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "        self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inpt, trgs, rand):\n",
    "        emb = self.emb(inpt)\n",
    "        batch_size = inpt.size(0)\n",
    "        rand = rand[:batch_size]\n",
    "        \n",
    "        ctx = self.ffw.weight[trgs.to(inpt.device)]\n",
    "        rnd = self.ffw.weight[rand.to(inpt.device)]\n",
    "        \n",
    "        out = torch.bmm(ctx.view(batch_size, 1, -1), emb.unsqueeze(2)).squeeze(2)\n",
    "        rnd = torch.bmm(rnd.view(batch_size, 1, -1), emb.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "        rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "        pst = -out.log().mean()\n",
    "        ngt = -(1 - rnd).log().mean()\n",
    "        \n",
    "        return pst + ngt\n",
    "\n",
    "# Load the model\n",
    "embedding_dim = 64\n",
    "model_path = \"finetuned_skipgram_model.pth\"\n",
    "mFoo = SkipGramFoo(len(updated_vocab), embedding_dim, 2).to(device)\n",
    "mFoo.load_state_dict(torch.load(model_path), strict=False)\n",
    "mFoo.eval()\n",
    "\n",
    "# Step 4: Generate embeddings for tokenized texts (query, passage, negative samples) with a progress bar\n",
    "def get_embeddings_for_tokens(tokenized_list, model):\n",
    "    embeddings_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tokens in tqdm(tokenized_list, desc=\"Generating embeddings\", unit=\"text\"):  # Add progress bar\n",
    "            if len(tokens) > 0:\n",
    "                token_tensor = torch.LongTensor(tokens).to(device)\n",
    "                token_embeddings = model.emb(token_tensor)  # Shape: [num_tokens, embedding_dim]\n",
    "                embeddings_list.append(token_embeddings.cpu().numpy())  # Keep the full sequence embeddings\n",
    "            else:\n",
    "                embeddings_list.append(torch.zeros((1, embedding_dim)).cpu().numpy())  # Zero vector for empty sequences\n",
    "    \n",
    "    return embeddings_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate embeddings for each title\n",
    "def get_embeddings_for_titles(tokenized_titles, model):\n",
    "    embeddings_list = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations for faster performance\n",
    "        for tokens in tokenized_titles:\n",
    "            if len(tokens) > 0:\n",
    "                # Move the tokens to the GPU\n",
    "                token_tensor = torch.LongTensor(tokens).to(device)\n",
    "                \n",
    "                # Get the embeddings for each token in the title\n",
    "                token_embeddings = model.emb(token_tensor)  # Shape: [num_tokens, embedding_dim]\n",
    "                \n",
    "                # Average the token embeddings to get a single vector for the entire title\n",
    "                title_embedding = token_embeddings.mean(dim=0)  # Shape: [embedding_dim]\n",
    "                \n",
    "                embeddings_list.append(title_embedding.cpu().numpy())  # Store the embedding as a NumPy array\n",
    "            else:\n",
    "                # Handle empty titles (if any)\n",
    "                embeddings_list.append(torch.zeros(embedding_dim).cpu().numpy())  # Zero vector for empty titles\n",
    "    \n",
    "    return embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results_negative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.iloc[1:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.5</th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>query</th>\n",
       "      <th>passage_text</th>\n",
       "      <th>negative_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>the inner of atomizer are surprisingly simple ...</td>\n",
       "      <td>or are herbal that promote greater density and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>based accountability also known as rba is way ...</td>\n",
       "      <td>find by star from the big dipper if you re fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>based accountability also known as rba is way ...</td>\n",
       "      <td>how to convert to one degree is equal rad the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>rba data driven decision making process to hel...</td>\n",
       "      <td>the hydrogen bond length of water with tempera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>identity manager risk based authentication rba...</td>\n",
       "      <td>pancreatitis is inflammation of the pancreas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>495</td>\n",
       "      <td>495</td>\n",
       "      <td>495</td>\n",
       "      <td>495</td>\n",
       "      <td>495</td>\n",
       "      <td>history of microchip timeline</td>\n",
       "      <td>illustration from jack inventor journal circui...</td>\n",
       "      <td>pathologist the under microscope to see if the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>history of microchip timeline</td>\n",
       "      <td>home computer history description are made up ...</td>\n",
       "      <td>solution there are in one inch set up the conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>497</td>\n",
       "      <td>497</td>\n",
       "      <td>497</td>\n",
       "      <td>497</td>\n",
       "      <td>497</td>\n",
       "      <td>history of microchip timeline</td>\n",
       "      <td>the history of rfid shown steady development i...</td>\n",
       "      <td>if you it approximately four to process refund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>history of microchip timeline</td>\n",
       "      <td>history of circuit the circuit otherwise known...</td>\n",
       "      <td>of the maximum and minimum the maximum and min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>history of microchip timeline</td>\n",
       "      <td>history of this history of is with the of tech...</td>\n",
       "      <td>although silverfish have creepy appearance and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.5  Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  \\\n",
       "3               3             3             3             3             3   \n",
       "4               4             4             4             4             4   \n",
       "5               5             5             5             5             5   \n",
       "6               6             6             6             6             6   \n",
       "7               7             7             7             7             7   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "495           495           495           495           495           495   \n",
       "496           496           496           496           496           496   \n",
       "497           497           497           497           497           497   \n",
       "498           498           498           498           498           498   \n",
       "499           499           499           499           499           499   \n",
       "\n",
       "     Unnamed: 0                          query  \\\n",
       "3             3                    what is rba   \n",
       "4             4                    what is rba   \n",
       "5             5                    what is rba   \n",
       "6             6                    what is rba   \n",
       "7             7                    what is rba   \n",
       "..          ...                            ...   \n",
       "495         495  history of microchip timeline   \n",
       "496         496  history of microchip timeline   \n",
       "497         497  history of microchip timeline   \n",
       "498         498  history of microchip timeline   \n",
       "499         499  history of microchip timeline   \n",
       "\n",
       "                                          passage_text  \\\n",
       "3    the inner of atomizer are surprisingly simple ...   \n",
       "4    based accountability also known as rba is way ...   \n",
       "5    based accountability also known as rba is way ...   \n",
       "6    rba data driven decision making process to hel...   \n",
       "7    identity manager risk based authentication rba...   \n",
       "..                                                 ...   \n",
       "495  illustration from jack inventor journal circui...   \n",
       "496  home computer history description are made up ...   \n",
       "497  the history of rfid shown steady development i...   \n",
       "498  history of circuit the circuit otherwise known...   \n",
       "499  history of this history of is with the of tech...   \n",
       "\n",
       "                                       negative_sample  \n",
       "3    or are herbal that promote greater density and...  \n",
       "4    find by star from the big dipper if you re fam...  \n",
       "5    how to convert to one degree is equal rad the ...  \n",
       "6    the hydrogen bond length of water with tempera...  \n",
       "7    pancreatitis is inflammation of the pancreas t...  \n",
       "..                                                 ...  \n",
       "495  pathologist the under microscope to see if the...  \n",
       "496  solution there are in one inch set up the conv...  \n",
       "497  if you it approximately four to process refund...  \n",
       "498  of the maximum and minimum the maximum and min...  \n",
       "499  although silverfish have creepy appearance and...  \n",
       "\n",
       "[497 rows x 9 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence  # For padding sequences\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import wandb  # Import Weights and Biases\n",
    "\n",
    "\n",
    "class TowerOneRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerOneRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=64, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass input through the RNN layer\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Unpack the output and get the last time step\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x = x[torch.arange(x.size(0)), lengths - 1]  # Get the last valid time step\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TowerTwoRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerTwoRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=64, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass input through the RNN layer\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # Unpack the output and get the last time step\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x = x[torch.arange(x.size(0)), lengths - 1]  # Get the last valid time step\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:87yr0j0x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>triplet_loss</td><td>█▇▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>triplet_loss</td><td>0.11852</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine_tuning_skipgram/packed_embedding</strong> at: <a href='https://wandb.ai/omareweis123/twotower_training/runs/87yr0j0x' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training/runs/87yr0j0x</a><br/> View project at: <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_163311-87yr0j0x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:87yr0j0x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\two_towers\\wandb\\run-20241025_163452-ckjltrrd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/twotower_training/runs/ckjltrrd' target=\"_blank\">fine_tuning_skipgram/packed_embedding</a></strong> to <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/twotower_training' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/twotower_training/runs/ckjltrrd' target=\"_blank\">https://wandb.ai/omareweis123/twotower_training/runs/ckjltrrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_21960\\4085510526.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_save_path)\n",
      "Epoch 1/30: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 completed. Loss: 0.9984116554260254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 completed. Loss: 0.9477606415748596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 completed. Loss: 0.9107734560966492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 completed. Loss: 0.8764935731887817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 completed. Loss: 0.8411028385162354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 completed. Loss: 0.8026745319366455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 completed. Loss: 0.7607420086860657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 completed. Loss: 0.7134956121444702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 completed. Loss: 0.664115846157074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 completed. Loss: 0.6160687804222107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 completed. Loss: 0.5746490955352783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 completed. Loss: 0.5393111109733582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 completed. Loss: 0.5077862739562988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 completed. Loss: 0.4739944636821747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 completed. Loss: 0.4429342746734619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 completed. Loss: 0.4128074645996094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 completed. Loss: 0.38203954696655273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 completed. Loss: 0.35317283868789673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 completed. Loss: 0.3216959834098816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 completed. Loss: 0.2921810448169708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 completed. Loss: 0.27183815836906433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 completed. Loss: 0.248849019408226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 completed. Loss: 0.2261599749326706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 completed. Loss: 0.20688356459140778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 completed. Loss: 0.18950271606445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 completed. Loss: 0.17165708541870117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 completed. Loss: 0.15961307287216187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 completed. Loss: 0.14617334306240082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 completed. Loss: 0.13972172141075134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 completed. Loss: 0.13416405022144318\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence  # For padding sequences\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import wandb  # Import Weights and Biases\n",
    "\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"twotower_training\", entity=\"omareweis123\", name='fine_tuning_skipgram/packed_embedding')\n",
    "\n",
    "# Load the saved SkipGram model\n",
    "model_save_path = \"finetuned_skipgram_model.pth\"\n",
    "checkpoint = torch.load(model_save_path)\n",
    "\n",
    "# Initialize the SkipGram model\n",
    "skipgram_model = SkipGramFoo(86996, 64, 2).to(device)  # Ensure to send the model to the correct device\n",
    "skipgram_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "skipgram_model.eval()  # Set the model to evaluation mode if you're not training it again\n",
    "\n",
    "# Tower RNN Models\n",
    "tower_one = TowerOneRNN().to(device)\n",
    "tower_two = TowerTwoRNN().to(device)\n",
    "\n",
    "# Triplet margin loss with cosine distance\n",
    "triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "    distance_function=lambda x, y: 1.0 - nn.functional.cosine_similarity(x, y),\n",
    "    margin=1.0,\n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "# Define optimizer for the tower models\n",
    "optimizer = optim.Adam(list(tower_one.parameters()) + list(tower_two.parameters()), lr=0.001)\n",
    "\n",
    "# Define query batch size\n",
    "query_batch_size = 128  # Number of queries to process in a batch\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 30  # Example\n",
    "\n",
    "# Track model and hyperparameters in W&B\n",
    "wandb.watch([tower_one, tower_two], log=\"all\")  # Log gradients and model weights\n",
    "\n",
    "# Training loop with batched queries\n",
    "for epoch in range(num_epochs):\n",
    "    # Group by 'query' to handle variable number of positives and negatives per query\n",
    "    query_groups = list(results.groupby('query'))\n",
    "\n",
    "    # Iterate through the dataset in batches of 'query_batch_size' queries\n",
    "    for q_batch_start in tqdm(range(0, len(query_groups), query_batch_size), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        query_batch = query_groups[q_batch_start:q_batch_start + query_batch_size]\n",
    "\n",
    "        all_anchor_embeddings = []\n",
    "        all_positive_embeddings = []\n",
    "        all_negative_embeddings = []\n",
    "\n",
    "        # Process each query group in the current batch\n",
    "        for query, group in query_batch:\n",
    "            # Tokenize the queries, passage_text, and negative_sample using your updated_vocab or reverse_vocab\n",
    "            query_tokens = tokenize_titles(group['query'].tolist(), reverse_vocab)\n",
    "            positive_tokens = tokenize_titles(group['passage_text'].tolist(), reverse_vocab)\n",
    "            negative_tokens = tokenize_titles(group['negative_sample'].tolist(), reverse_vocab)\n",
    "\n",
    "            # Get embeddings for each group\n",
    "            anchor_embeddings = get_embeddings_for_titles(query_tokens, skipgram_model)\n",
    "            positive_embeddings = get_embeddings_for_titles(positive_tokens, skipgram_model)\n",
    "            negative_embeddings = get_embeddings_for_titles(negative_tokens, skipgram_model)\n",
    "\n",
    "            # Append the embeddings to the lists\n",
    "            all_anchor_embeddings.append(torch.tensor(anchor_embeddings))\n",
    "            all_positive_embeddings.append(torch.tensor(positive_embeddings))\n",
    "            all_negative_embeddings.append(torch.tensor(negative_embeddings))\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "            anchor_batch = pad_sequence(all_anchor_embeddings, batch_first=True).to(device)\n",
    "            positive_batch = pad_sequence(all_positive_embeddings, batch_first=True).to(device)\n",
    "            negative_batch = pad_sequence(all_negative_embeddings, batch_first=True).to(device)\n",
    "\n",
    "            # Get lengths of the original sequences and move them to CPU\n",
    "            anchor_lengths = torch.tensor([len(seq) for seq in all_anchor_embeddings]).cpu()  # Move to CPU\n",
    "            positive_lengths = torch.tensor([len(seq) for seq in all_positive_embeddings]).cpu()  # Move to CPU\n",
    "            negative_lengths = torch.tensor([len(seq) for seq in all_negative_embeddings]).cpu()  # Move to CPU\n",
    "\n",
    "            # Forward pass through the two towers\n",
    "            anchor_output = tower_one(anchor_batch, anchor_lengths)\n",
    "            positive_output = tower_two(positive_batch, positive_lengths)\n",
    "            negative_output = tower_two(negative_batch, negative_lengths)\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss to W&B\n",
    "        wandb.log({\"epoch\": epoch + 1, \"triplet_loss\": triplet_loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {triplet_loss.item()}\")\n",
    "\n",
    "    # Optionally, save a model checkpoint after every epoch and log it to W&B\n",
    "    torch.save({\n",
    "        'model_state_dict': skipgram_model.state_dict(),\n",
    "        'tower_one_state_dict': tower_one.state_dict(),\n",
    "        'tower_two_state_dict': tower_two.state_dict(),\n",
    "    }, f\"model_1_checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    wandb.finish\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doing query analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_21960\\1597896442.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"model_1_checkpoint_epoch_30.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TowerTwoRNN(\n",
       "  (rnn): RNN(64, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading in tower two to create database of text_passages \n",
    "\n",
    "tower_two = TowerTwoRNN()\n",
    "\n",
    "# Load the saved TowerTwo model weights\n",
    "checkpoint = torch.load(\"model_1_checkpoint_epoch_30.pth\")\n",
    "tower_two.load_state_dict(checkpoint['tower_two_state_dict'])\n",
    "\n",
    "# Set TowerTwo to evaluation mode\n",
    "tower_two.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86996"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_21960\\1800973642.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_save_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkipGramFoo(\n",
       "  (emb): Embedding(86996, 64)\n",
       "  (ffw): Linear(in_features=64, out_features=86996, bias=False)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary models and device are already set up\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the saved SkipGram model\n",
    "model_save_path = \"finetuned_skipgram_model.pth\"\n",
    "checkpoint = torch.load(model_save_path)\n",
    "\n",
    "# Initialize the SkipGram model\n",
    "skipgram_model = SkipGramFoo(86996, 64, 2).to(device)  # Ensure to send the model to the correct device\n",
    "skipgram_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "skipgram_model.eval()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np  # Import the tqdm library for progress bars\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to create a reverse vocabulary from the updated vocabulary\n",
    "def create_reverse_vocab(vocab):\n",
    "    return {word: index for index, word in vocab.items()}\n",
    "\n",
    "# Load the reverse vocabulary\n",
    "reverse_vocab = create_reverse_vocab(updated_vocab)\n",
    "\n",
    "# Function to tokenize a list of texts using the reverse vocabulary\n",
    "def tokenize_texts(texts, reverse_vocab):\n",
    "    tokens_list = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.lower().split()  # Convert the text to lowercase to match training preprocessing\n",
    "        \n",
    "        tokenized = []\n",
    "        for word in words:\n",
    "            if word in reverse_vocab:\n",
    "                tokenized.append(reverse_vocab[word])  # Get the index from reverse_vocab\n",
    "            # No else clause needed; unknown words are simply skipped\n",
    "        \n",
    "        tokens_list.append(tokenized)\n",
    "    \n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "# Step 2: Tokenize the query, passage_text, and negative_samples columns\n",
    "tokenized_queries = tokenize_texts(results['passage_text'].tolist(), reverse_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_21960\\3451731986.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mFoo.load_state_dict(torch.load(model_path), strict=False)\n",
      "Generating embeddings: 100%|██████████| 497/497 [00:00<00:00, 6867.62text/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embeddings saved to 'query_embeddings.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Define the SkipGramFoo model\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "    def __init__(self, voc, emb, ctx):\n",
    "        super().__init__()\n",
    "        self.ctx = ctx\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "        self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inpt, trgs, rand):\n",
    "        emb = self.emb(inpt)\n",
    "        batch_size = inpt.size(0)\n",
    "        rand = rand[:batch_size]\n",
    "        \n",
    "        ctx = self.ffw.weight[trgs.to(inpt.device)]\n",
    "        rnd = self.ffw.weight[rand.to(inpt.device)]\n",
    "        \n",
    "        out = torch.bmm(ctx.view(batch_size, 1, -1), emb.unsqueeze(2)).squeeze(2)\n",
    "        rnd = torch.bmm(rnd.view(batch_size, 1, -1), emb.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "        rnd = self.sig(rnd).clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "        pst = -out.log().mean()\n",
    "        ngt = -(1 - rnd).log().mean()\n",
    "        \n",
    "        return pst + ngt\n",
    "\n",
    "# Load the model\n",
    "embedding_dim = 64\n",
    "model_path = \"finetuned_skipgram_model.pth\"\n",
    "mFoo = SkipGramFoo(len(updated_vocab), embedding_dim, 2).to(device)\n",
    "mFoo.load_state_dict(torch.load(model_path), strict=False)\n",
    "mFoo.eval()\n",
    "\n",
    "# Step 4: Generate embeddings for tokenized texts (query, passage, negative samples) with a progress bar\n",
    "def get_embeddings_for_tokens(tokenized_list, model):\n",
    "    embeddings_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tokens in tqdm(tokenized_list, desc=\"Generating embeddings\", unit=\"text\"):  # Add progress bar\n",
    "            if len(tokens) > 0:\n",
    "                token_tensor = torch.LongTensor(tokens).to(device)\n",
    "                token_embeddings = model.emb(token_tensor)  # Shape: [num_tokens, embedding_dim]\n",
    "                embeddings_list.append(token_embeddings.cpu().numpy())  # Keep the full sequence embeddings\n",
    "            else:\n",
    "                embeddings_list.append(torch.zeros((1, embedding_dim)).cpu().numpy())  # Zero vector for empty sequences\n",
    "    \n",
    "    return embeddings_list\n",
    "\n",
    "query_embeddings = get_embeddings_for_tokens(tokenized_queries, mFoo)\n",
    "\n",
    "# Save embeddings as a pickle file\n",
    "with open('passage_text.pkl', 'wb') as f:\n",
    "    pickle.dump(query_embeddings, f)\n",
    "\n",
    "print(\"Query embeddings saved to 'query_embeddings.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 8/8 [00:00<00:00, 103.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ensure your model is on the GPU\n",
    "tower_two = tower_two.to(device)  # Move the model to the specified device\n",
    "BATCH_SIZE = 64 \n",
    "tower_two_outputs = []\n",
    "# Process embeddings in batches\n",
    "for i in tqdm(range(0, len(query_embeddings), BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "    # Get the current batch of embeddings\n",
    "    batch_embeddings = query_embeddings[i:i + BATCH_SIZE]\n",
    "\n",
    "    # Convert the batch to tensors\n",
    "    embeddings_tensors = [torch.tensor(emb) for emb in batch_embeddings]\n",
    "\n",
    "    # Pad the current batch of tensors\n",
    "    padded_embeddings = pad_sequence(embeddings_tensors, batch_first=True).to(device)\n",
    "    \n",
    "    # Get lengths of the original sequences and move to CPU\n",
    "    lengths = torch.tensor([len(seq) for seq in embeddings_tensors]).cpu()  # Move lengths tensor to CPU\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        batch_outputs = tower_two(padded_embeddings, lengths)\n",
    "\n",
    "    # Append results\n",
    "    tower_two_outputs.append(batch_outputs)\n",
    "\n",
    "# Combine results if needed\n",
    "tower_two_outputs = torch.cat(tower_two_outputs, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([497, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tower_two_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_21960\\2750994381.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"model_checkpoint_epoch_5.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TowerOneRNN(\n",
       "  (rnn): RNN(64, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading in tower two to create database of text_passages \n",
    "\n",
    "tower_one = TowerOneRNN()\n",
    "\n",
    "# Load the saved TowerTwo model weights\n",
    "checkpoint = torch.load(\"model_checkpoint_epoch_5.pth\")\n",
    "tower_one.load_state_dict(checkpoint['tower_one_state_dict'])\n",
    "\n",
    "# Set TowerTwo to evaluation mode\n",
    "tower_one.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TowerOne and TowerTwo RNN models\n",
    "class TowerOneRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TowerOneRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=64, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.rnn(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x = x[torch.arange(x.size(0)), lengths - 1]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omare\\AppData\\Local\\Temp\\ipykernel_21960\\3633070367.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"model_checkpoint_epoch_5.pth\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to create a reverse vocabulary from the updated vocabulary\n",
    "def create_reverse_vocab(vocab):\n",
    "    return {word: index for index, word in vocab.items()}\n",
    "\n",
    "# Load the reverse vocabulary\n",
    "reverse_vocab = create_reverse_vocab(updated_vocab)\n",
    "\n",
    "# Function to tokenize a single text using the reverse vocabulary\n",
    "def tokenize_text(text, reverse_vocab):\n",
    "    words = text.lower().split()  # Convert the text to lowercase to match training preprocessing\n",
    "    tokenized = []\n",
    "    for word in words:\n",
    "        if word in reverse_vocab:\n",
    "            tokenized.append(reverse_vocab[word])  # Get the index from reverse_vocab\n",
    "    return tokenized\n",
    "\n",
    "# Input your query\n",
    "single_query = \"what is rba\"  # Repl with your actual query text\n",
    "tokenized_query = tokenize_text(single_query, reverse_vocab)\n",
    "\n",
    "# Load the saved TowerTwo model weights\n",
    "checkpoint = torch.load(\"model_checkpoint_epoch_5.pth\")\n",
    "tower_one.load_state_dict(checkpoint['tower_one_state_dict'])\n",
    "\n",
    "# Set TowerTwo to evaluation mode\n",
    "tower_one.eval()\n",
    "\n",
    "def get_embedding_for_single_token(tokens, model):\n",
    "    with torch.no_grad():\n",
    "        if len(tokens) > 0:\n",
    "            token_tensor = torch.LongTensor(tokens).to(device)\n",
    "            token_embedding = model.emb(token_tensor)  # Shape: [num_tokens, embedding_dim]\n",
    "            return token_embedding.cpu().numpy()  # Return the full sequence embeddings\n",
    "        else:\n",
    "            return torch.zeros((1, embedding_dim)).cpu().numpy()  # Zero vector for empty sequences\n",
    "\n",
    "\n",
    "query_embeddings = get_embedding_for_single_token(tokenized_query, mFoo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from tower_one: tensor([[-0.3574, -1.3020, -0.2408]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Ensure your model is on the GPU\n",
    "tower_one = tower_one.to(device)  # Move the model to the specified device\n",
    "\n",
    "# Assuming 'query_embedding' is the embedding for the single query\n",
    "query_embedding_tensor = torch.tensor(query_embeddings).unsqueeze(0).to(device)  # Add a batch dimension\n",
    "\n",
    "# Get the length of the original sequence\n",
    "length = torch.tensor([len(query_embeddings)]).cpu()  # Move lengths tensor to CPU\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    tower_one_output = tower_one(query_embedding_tensor, length)\n",
    "\n",
    "# Process the output as needed\n",
    "# You can now use 'tower_one_output' for further processing\n",
    "print(\"Output from tower_one:\", tower_one_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([497, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tower_two_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tower_one_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar index: 187\n",
      "Cosine similarity value: 0.9925868511199951\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure your outputs are on the same device (GPU/CPU)\n",
    "tower_one_output = tower_one_output.to(device)\n",
    "tower_two_output = tower_two_outputs.to(device)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "# tower_one_output is (1, 3), so we need to repeat it for each of the 60000 rows\n",
    "similarities = F.cosine_similarity(tower_one_output, tower_two_output, dim=1)\n",
    "\n",
    "# Find the index of the maximum similarity\n",
    "most_similar_index = torch.argmax(similarities)\n",
    "\n",
    "# Print the result\n",
    "print(\"Most similar index:\", most_similar_index.item())  # Convert to Python integer if needed\n",
    "print(\"Cosine similarity value:\", similarities[most_similar_index].item())  # Convert to Python float if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar passage text: never seen pound cake take less than an hour most take min and have one recipe that for those long times tho are for slow oven about till toothpick inserted in the center of the cake comes out clean on the heat of your oven but start at about and when the cake is golden brown ago up\n"
     ]
    }
   ],
   "source": [
    "# Assuming results_df is your DataFrame containing the 'passage_text' column\n",
    "\n",
    "# Get the most similar index from the cosine similarity results\n",
    "most_similar_index = most_similar_index.item()  # Ensure it's a Python integer\n",
    "\n",
    "# Retrieve the passage_text from results_df using the most similar index\n",
    "similar_passage_text = results['passage_text'].iloc[most_similar_index]\n",
    "\n",
    "# Print the passage text\n",
    "print(\"Most similar passage text:\", similar_passage_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar index 187:\n",
      "Cosine similarity value: 0.9925868511199951\n",
      "Passage text: never seen pound cake take less than an hour most take min and have one recipe that for those long times tho are for slow oven about till toothpick inserted in the center of the cake comes out clean on the heat of your oven but start at about and when the cake is golden brown ago up\n",
      "\n",
      "Most similar index 342:\n",
      "Cosine similarity value: 0.9874629378318787\n",
      "Passage text: as baby girl name is currently not popular baby name in the usa the following chart the popularity of the name in the usa over the past usa baby name statistics you need the adobe flash player and browser with to see all baby name popularity the following are baby related to and are suitable for and of ona both\n",
      "\n",
      "Most similar index 341:\n",
      "Cosine similarity value: 0.9874410033226013\n",
      "Passage text: the name the following meaning my father joy an form of in turn variant of abigail also variant of the name gala meaning merry maker the following are baby related to and are suitable for and of ona both\n",
      "\n",
      "Most similar index 229:\n",
      "Cosine similarity value: 0.9724524021148682\n",
      "Passage text: photosynthesis is process used by and other to convert light energy normally from the sun into chemical energy that can be later to fuel the however not all that use light as source of energy carry out photosynthesis since use organic rather than carbon dioxide as source of carbon in algae and photosynthesis oxygen\n",
      "\n",
      "Most similar index 481:\n",
      "Cosine similarity value: 0.9701740741729736\n",
      "Passage text: range the range is another range used as measure of the spread the difference between upper and lower which is the range also the dispersion of data set the range of data set and the influence of because in effect the highest and quarters are removed semi quartile range the semi quartile range is another measure of spread it is calculated as one half the difference between the percentile often and the percentile the formula for semi quartile range is\n",
      "\n",
      "Most similar index 449:\n",
      "Cosine similarity value: 0.9692336320877075\n",
      "Passage text: atm basic hydraulics and pneumatics module introduction to pneumatics dental drill is one of the of the pneumatic technology pneumatic device is used to fill the tire with compressed air to adjust the tire pressure atm basic hydraulics and pneumatics\n",
      "\n",
      "Most similar index 131:\n",
      "Cosine similarity value: 0.9661521911621094\n",
      "Passage text: get an instant estimate of the cost to install concrete pad our free calculator recent data to estimate for your concrete pad installation project for basic square project in zip code the cost to install concrete pad between per square foot to estimate for your project\n",
      "\n",
      "Most similar index 384:\n",
      "Cosine similarity value: 0.9614832401275635\n",
      "Passage text: child life with than three of experience in the field those with four to nine of experience an average of and those with nine or more of experience those with nine or more of experience as of child life an average of per year while those with only bachelor an average of child life with master\n",
      "\n",
      "Most similar index 390:\n",
      "Cosine similarity value: 0.9614832401275635\n",
      "Passage text: as of the child life council that or of child life an average annual salary of those who bachelor average earnings of per year while those who master received an average of per year those with nine or more of experience as of child life an average of per year while those with only bachelor an average of child life with master\n",
      "\n",
      "Most similar index 441:\n",
      "Cosine similarity value: 0.9602086544036865\n",
      "Passage text: pneumatic tool air tool air powered tool or pneumatic powered tool is type of power tool driven by compressed air by an air compressor pneumatic can also be driven by compressed carbon dioxide co in small for portability\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure your outputs are on the same device (GPU/CPU)\n",
    "tower_one_output = tower_one_output.to(device)\n",
    "tower_two_output = tower_two_outputs.to(device)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarities = F.cosine_similarity(tower_one_output, tower_two_output, dim=1)\n",
    "\n",
    "# Get the top 5 most similar indices\n",
    "top_k = 10\n",
    "top_k_indices = torch.topk(similarities, top_k).indices.cpu()  # Move to CPU\n",
    "\n",
    "# Retrieve the top 5 passages from results_df\n",
    "top_passages = results['passage_text'].iloc[top_k_indices.numpy()]  # Convert to NumPy array\n",
    "\n",
    "# Print the results\n",
    "for i in range(top_k):\n",
    "    print(f\"Most similar index {top_k_indices[i].item()}:\")\n",
    "    print(\"Cosine similarity value:\", similarities[top_k_indices[i]].item())\n",
    "    print(\"Passage text:\", top_passages.iloc[i])\n",
    "    print()  # Add a newline for better readability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
